{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd0c4b7",
   "metadata": {},
   "source": [
    "<h1>Building a Neural Network  from scratch </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d991b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0814c",
   "metadata": {},
   "source": [
    "<b>Step 1: Initializing Parameters</b>\n",
    "\n",
    "First, we need to initialize the weights and biases for each layer. Small random values are typically used for weights, and biases can be initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42cc7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2b887",
   "metadata": {},
   "source": [
    "<b>Step 2: Implement Forward Propagation</b>\n",
    "\n",
    "Forward propagation is the process of calculating the output of the neural network for a given input. We need to calculate the linear part and then apply the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2182256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca1afc2",
   "metadata": {},
   "source": [
    "<b>Activation functions:</b> Activation functions introduce non-linearities into the network, enabling it to learn complex data patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cabded",
   "metadata": {},
   "source": [
    "<b>a. Sigmoid Function</b>\n",
    "\n",
    "The sigmoid function is traditionally used for binary classification tasks, especially in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1aeecab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    S = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * S * (1 - S)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b29f0",
   "metadata": {},
   "source": [
    "<b>b. ReLU Function</b>\n",
    "\n",
    "ReLU (Rectified Linear Unit) is widely used in hidden layers due to its efficiency and effectiveness in addressing the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8f2df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b12b0",
   "metadata": {},
   "source": [
    "<b>c. Tanh Function</b>\n",
    "\n",
    "The tanh function is a scaled version of the sigmoid and can be more effective in certain hidden layers due to its output range of [-1, 1], centering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9389033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    A = np.tanh(Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def tanh_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = dA * (1 - np.tanh(Z)**2)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca78f81b",
   "metadata": {},
   "source": [
    "<b>d. Softmax Function</b>\n",
    "\n",
    "Softmax is typically used in the output layer of a multiclass classification network, converting logits to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5ef17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    A = expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax_backward(Y, cache):\n",
    "    Z = cache\n",
    "    A, _ = softmax(Z)\n",
    "    dZ = A - Y\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de642a",
   "metadata": {},
   "source": [
    "<b>Step 3: Compute the Loss</b>\n",
    "\n",
    "The loss function measures the performance of the neural network. The choice of loss function depends on the task (e.g., cross-entropy for classification, mean squared error for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e69b20",
   "metadata": {},
   "source": [
    "<b>a. Mean Squared Error (MSE) - For Regression</b>\n",
    "\n",
    "Used primarily in regression tasks, where the goal is to predict continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56368837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(Y, Y_hat):\n",
    "    m = Y.shape[1]\n",
    "    cost = (1 / (2 * m)) * np.sum(np.square(Y_hat - Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdedaf",
   "metadata": {},
   "source": [
    "<b>b. Binary Cross-Entropy Loss - For Binary Classification</b>\n",
    "\n",
    "Commonly used in binary classification tasks. It measures the performance of a classification model whose output is a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cabd3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(Y, Y_hat):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))\n",
    "    cost = np.squeeze(cost)  # To ensure the cost is the proper shape (e.g., turns [[17]] into 17).\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5a8fa",
   "metadata": {},
   "source": [
    "<b>c. Categorical Cross-Entropy Loss - For Multiclass Classification</b>\n",
    "\n",
    "Used in multiclass classification settings, where the goal is to categorize instances into more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64078d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(Y, Y_hat):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(Y_hat))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d10a1",
   "metadata": {},
   "source": [
    "<b>Step 4: Backward Propagation</b>\n",
    "\n",
    "Backward propagation calculates the gradient of the loss function with respect to the parameters, which is used to update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cce44b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e756da0",
   "metadata": {},
   "source": [
    "<b>Step 5: Update Parameters</b>\n",
    "\n",
    "Using the gradients computed from backpropagation, we update the weights and biases of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0446af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d648d",
   "metadata": {},
   "source": [
    "<b>Step 6: Training the Neural Network</b>\n",
    "\n",
    "After defining all necessary components, we need to put them together to train the neural network. Training involves feeding the network with data, performing forward and backward propagation, and updating the model's weights and biases iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4165335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    " \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Printing the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(f\"Learning rate = {learning_rate}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9432b7",
   "metadata": {},
   "source": [
    "<b>Step 7: Evaluation and Prediction</b>\n",
    "\n",
    "Once the model is trained, we can use it to make predictions on new data. To do this, we simply perform forward propagation with the learned parameters and interpret the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2879d9",
   "metadata": {},
   "source": [
    "<b>a. Binary Classification (Sigmoid Activation)</b>\n",
    "\n",
    "This is the simplest case, typically used when the output layer of the neural network uses a sigmoid activation function for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3858ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary(X, parameters):\n",
    "    AL, _ = forward_propagation(X, parameters)\n",
    "    predictions = AL > 0.5  # Default threshold is 0.5\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3623a0",
   "metadata": {},
   "source": [
    "<b>b. Binary Classification with Custom Threshold</b>\n",
    "\n",
    "In some applications, especially where there is class imbalance or different costs associated with false positives and false negatives, we may want to adjust the decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb966170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary_threshold(X, parameters, threshold=0.5):\n",
    "    AL, _ = forward_propagation(X, parameters)\n",
    "    predictions = AL > threshold\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27601150",
   "metadata": {},
   "source": [
    "<b>c. Multiclass Classification (Softmax Activation)</b>\n",
    "\n",
    "For multiclass classification, the output layer typically uses the softmax activation. Here, the prediction is based on the index of the maximum output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07c54b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multiclass(X, parameters):\n",
    "    AL, _ = forward_propagation(X, parameters)\n",
    "    predictions = np.argmax(AL, axis=0)\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
